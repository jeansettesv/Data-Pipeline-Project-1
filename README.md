# ğŸš€ Data Pipeline - MinIO & PostgreSQL

## ğŸ“Œ VisÃ£o Geral
Este projeto implementa um **pipeline de dados** para processar transaÃ§Ãµes fictÃ­cias.  
Os dados sÃ£o armazenados no **MinIO** (como Data Lake) e carregados no **PostgreSQL**.  
O ambiente pode ser configurado **tanto com Docker quanto com scripts manuais**.

---

## ğŸ“‚ Estrutura do Projeto
```bash
meu-projeto/
â”œâ”€â”€ data/                    # ğŸ“‚ DiretÃ³rio principal de dados
â”‚   â”œâ”€â”€ source/              # ğŸ“‚ Arquivos gerados antes do upload para o MinIO
â”‚   â”œâ”€â”€ extracted/           # ğŸ“‚ Arquivos extraÃ­dos do MinIO
â”‚   â”œâ”€â”€ transformed/         # ğŸ“‚ Arquivos transformados, prontos para carga no PostgreSQL
â”œâ”€â”€ init_db/                 # ğŸ“‚ Backup inicial do banco de dados
â”‚   â”œâ”€â”€ banco_inicial_bkp.sql  # ğŸ—„ï¸ Arquivo SQL para restaurar a base de clientes
â”œâ”€â”€ scripts/                 # ğŸ“‚ Scripts do pipeline ETL
â”‚   â”œâ”€â”€ restore_db.py         # ğŸ”„ Restaura o banco antes de rodar o pipeline
â”‚   â”œâ”€â”€ extract.py            # ğŸ”„ Extrai transaÃ§Ãµes do MinIO
â”‚   â”œâ”€â”€ transform.py          # ğŸ”„ Processa os dados extraÃ­dos
â”‚   â”œâ”€â”€ load.py               # ğŸ”„ Carrega transaÃ§Ãµes no banco
â”‚   â”œâ”€â”€ upload_to_minio.py    # ğŸ”„ Envia os arquivos para o MinIO
â”œâ”€â”€ utils/                    # ğŸ› ï¸ Scripts auxiliares
â”‚   â”œâ”€â”€ generate_clients.py   # ğŸ”„ Gera clientes a partir de transacoes.csv
â”‚   â”œâ”€â”€ load_clientes.py      # ğŸ”„ Carrega clientes no banco
â”‚   â”œâ”€â”€ generate_data.py      # ğŸ”„ Gera transaÃ§Ãµes fictÃ­cias
â”‚   â”œâ”€â”€ backup_db.py          # ğŸ”„ Cria um backup do banco
â”œâ”€â”€ docker-compose.yml        # ğŸ³ ConfiguraÃ§Ã£o para rodar no Docker
â”œâ”€â”€ README.md                 # ğŸ“„ DocumentaÃ§Ã£o do projeto
```
---

## âš™ï¸ **ConfiguraÃ§Ã£o e Uso**

### ğŸ”¹ **OpÃ§Ã£o 1: ConfiguraÃ§Ã£o via Docker**
Se quiser rodar tudo com Docker, basta executar:

```bash
docker-compose up -d
```
âœ… Isso iniciarÃ¡ o PostgreSQL e o MinIO automaticamente, com os clientes jÃ¡ carregados no banco.

---

### ğŸ”¹ **OpÃ§Ã£o 2: ConfiguraÃ§Ã£o Manual**
Caso prefira rodar sem Docker, siga os passos:

1ï¸âƒ£ Restaurar o banco de dados (opcional se usar Docker)

```bash
python scripts/restore_db.py
```
ğŸ“Œ Isso restaura um backup do banco contendo a tabela de clientes.
âš ï¸ NÃ£o execute este comando se tiver gerado novos dados de transaÃ§Ãµes, pois o backup ficaria desatualizado!

2ï¸âƒ£ Rodar o pipeline de ETL

```bash
python scripts/upload_to_minio.py  # Envia o arquivo de transaÃ§Ãµes da pasta source para o MinIO
python scripts/extract.py          # Extrai o arquivo transaÃ§Ãµes do MinIO e salva em .csv
python scripts/transform.py        # Processa e transforma os dados extraÃ­dos e salva em .csv
python scripts/load.py             # Carrega as transaÃ§Ãµes transformadas no banco de dados
python scripts/create_enriched_table.py  # Cria a tabela transacoes_enriquecidas com JOIN entre clientes e transaÃ§Ãµes
python scripts/export_parquet.py   # Exporta os dados enriquecidos em formato .parquet
```
ğŸ“Œ Ao final deste processo, serÃ¡ gerado o arquivo `transacoes_enriquecidas.parquet` na pasta `data/transformed/`.

3ï¸âƒ£ Caso queira mudar os dados

Se quiser modificar os dados e reiniciar o pipeline, execute os scripts abaixo antes de rodar o ETL novamente:

```bash
python utils/generate_data.py     # Gera um novo arquivo transacoes.csv com transaÃ§Ãµes fictÃ­cias  
python utils/generate_clients.py  # Gera um novo arquivo clientes.csv baseado nos IDs do arquivo de transaÃ§Ãµes  
python utils/load_clientes.py     # Carrega os novos clientes no banco  
python utils/backup_db.py         # (Opcional) Gera um novo backup do banco apenas com os clientes recÃ©m-carregados  
```
ğŸ“Œ Depois, volte para a etapa do pipeline de ETL ou restaure o backup antes de prosseguir.

---

## ğŸ“¦ ExportaÃ§Ã£o Final

ApÃ³s carregar os dados no banco, o script `export_parquet.py` exporta a tabela `transacoes_enriquecidas` em formato `.parquet` para a pasta `data/transformed/`.  
Esse arquivo pode ser enviado para uma camada de analytics como **Databricks, BigQuery ou Amazon Athena**.


